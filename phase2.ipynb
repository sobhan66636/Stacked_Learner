{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9673b38b",
   "metadata": {},
   "source": [
    "# Phase 2 : Stack Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8963c719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 606 labels for 3 classes: ['cats', 'dogs', 'horses']\n",
      "Loaded early features: (606, 802816)\n",
      "Loaded middle features: (606, 802816)\n",
      "Loaded high features: (606, 100352)\n",
      "\n",
      "==================================================\n",
      "PHASE 2 - STACKED LEARNER FOR EARLY FEATURES\n",
      "==================================================\n",
      "PCA reduced features from 802816 to 282\n",
      "\n",
      "Tuning and selecting Logistic Regression with strict overfit prevention...\n",
      "Testing 8 parameter combinations...\n",
      "  Combo 1/8: Gap=0.4907, Val F1=0.5060\n",
      "Selected params: {'C': 0.001, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Selected Logistic Regression model val F1: 0.5625, train-val acc gap: 0.4376\n",
      "\n",
      "Tuning and selecting Random Forest with strict overfit prevention...\n",
      "Testing 216 parameter combinations...\n",
      "  Combo 1/216: Gap=0.2913, Val F1=0.5284\n",
      "  Combo 10/216: Gap=0.2717, Val F1=0.5153\n",
      "  Combo 20/216: Gap=0.3110, Val F1=0.4929\n",
      "  Combo 30/216: Gap=0.2536, Val F1=0.5305\n",
      "  Combo 40/216: Gap=0.4242, Val F1=0.4897\n",
      "  Combo 50/216: Gap=0.3719, Val F1=0.5311\n",
      "  Combo 60/216: Gap=0.3869, Val F1=0.4860\n",
      "  Combo 70/216: Gap=0.3621, Val F1=0.5110\n",
      "  Combo 80/216: Gap=0.4732, Val F1=0.4620\n",
      "  Combo 90/216: Gap=0.3955, Val F1=0.5412\n",
      "  Combo 100/216: Gap=0.4051, Val F1=0.4956\n",
      "  Combo 110/216: Gap=0.3325, Val F1=0.5396\n",
      "  Combo 120/216: Gap=0.3357, Val F1=0.4897\n",
      "  Combo 130/216: Gap=0.2882, Val F1=0.5352\n",
      "  Combo 140/216: Gap=0.3336, Val F1=0.4819\n",
      "  Combo 150/216: Gap=0.3977, Val F1=0.5270\n",
      "  Combo 160/216: Gap=0.4111, Val F1=0.5114\n",
      "  Combo 170/216: Gap=0.3553, Val F1=0.5406\n",
      "  Combo 180/216: Gap=0.4024, Val F1=0.5022\n",
      "  Combo 190/216: Gap=0.3910, Val F1=0.5356\n",
      "  Combo 200/216: Gap=0.4623, Val F1=0.5042\n",
      "  Combo 210/216: Gap=0.3501, Val F1=0.5760\n",
      "Selected params: {'n_estimators': 50, 'max_depth': 3, 'min_samples_split': 30, 'min_samples_leaf': 10, 'max_features': 'sqrt', 'max_samples': 0.6}\n",
      "Selected Random Forest model val F1: 0.5553, train-val acc gap: 0.2283\n",
      "\n",
      "Tuning and selecting SVM (RBF) with strict overfit prevention...\n",
      "Testing 9 parameter combinations...\n",
      "  Combo 1/9: Gap=0.0234, Val F1=0.2445\n",
      "Selected params: {'C': 0.1, 'gamma': 'scale'}\n",
      "Selected SVM (RBF) model val F1: 0.3821, train-val acc gap: 0.0279\n",
      "\n",
      "Tuning and selecting Gradient Boosting with strict overfit prevention...\n",
      "Testing 216 parameter combinations...\n",
      "  Combo 1/216: Gap=0.2008, Val F1=0.5539\n",
      "  Combo 10/216: Gap=0.1802, Val F1=0.5510\n",
      "  Combo 20/216: Gap=0.2984, Val F1=0.5881\n",
      "  Combo 30/216: Gap=0.3330, Val F1=0.5861\n",
      "  Combo 40/216: Gap=0.3362, Val F1=0.5622\n",
      "  Combo 50/216: Gap=0.3805, Val F1=0.5924\n",
      "  Combo 60/216: Gap=0.3853, Val F1=0.5934\n",
      "  Combo 70/216: Gap=0.3955, Val F1=0.5958\n",
      "  Combo 80/216: Gap=0.4121, Val F1=0.5564\n",
      "  Combo 90/216: Gap=0.4127, Val F1=0.5827\n",
      "  Combo 100/216: Gap=0.4070, Val F1=0.5921\n",
      "  Combo 110/216: Gap=0.2541, Val F1=0.5734\n",
      "  Combo 120/216: Gap=0.2324, Val F1=0.5723\n",
      "  Combo 130/216: Gap=0.3279, Val F1=0.5964\n",
      "  Combo 140/216: Gap=0.3634, Val F1=0.6087\n",
      "  Combo 150/216: Gap=0.3873, Val F1=0.5865\n",
      "  Combo 160/216: Gap=0.3811, Val F1=0.6112\n",
      "  Combo 170/216: Gap=0.4028, Val F1=0.5935\n",
      "  Combo 180/216: Gap=0.3925, Val F1=0.6028\n",
      "  Combo 190/216: Gap=0.4121, Val F1=0.5827\n",
      "  Combo 200/216: Gap=0.4255, Val F1=0.5728\n",
      "  Combo 210/216: Gap=0.4235, Val F1=0.5740\n",
      "Selected params: {'n_estimators': 50, 'learning_rate': 0.01, 'max_depth': 2, 'subsample': 0.8, 'min_samples_leaf': 15, 'min_samples_split': 10}\n",
      "Selected Gradient Boosting model val F1: 0.5633, train-val acc gap: 0.1703\n",
      "\n",
      "Training stacked learner...\n",
      "PCA reduced features from 802816 to 282\n",
      "Training base learner: Logistic Regression\n",
      "Training base learner: Random Forest\n",
      "Training base learner: SVM (RBF)\n",
      "Training base learner: Gradient Boosting\n",
      "\n",
      "Stacked Learner Results for early features:\n",
      "Train Accuracy: 0.9236\n",
      "Test Accuracy: 0.6148\n",
      "Test F1 Macro: 0.5564\n",
      "Overfitting Gap: 0.3088\n",
      "Base learners used: Logistic Regression, Random Forest, SVM (RBF), Gradient Boosting\n",
      "\n",
      "==================================================\n",
      "PHASE 2 - STACKED LEARNER FOR MIDDLE FEATURES\n",
      "==================================================\n",
      "PCA reduced features from 802816 to 344\n",
      "\n",
      "Tuning and selecting Logistic Regression with strict overfit prevention...\n",
      "Testing 8 parameter combinations...\n",
      "  Combo 1/8: Gap=0.2396, Val F1=0.7623\n",
      "Selected params: {'C': 0.001, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Selected Logistic Regression model val F1: 0.7806, train-val acc gap: 0.2190\n",
      "\n",
      "Tuning and selecting Random Forest with strict overfit prevention...\n",
      "Testing 216 parameter combinations...\n",
      "  Combo 1/216: Gap=0.3016, Val F1=0.6078\n",
      "  Combo 10/216: Gap=0.2318, Val F1=0.6669\n",
      "  Combo 20/216: Gap=0.3217, Val F1=0.5750\n",
      "  Combo 30/216: Gap=0.2459, Val F1=0.6518\n",
      "  Combo 40/216: Gap=0.4028, Val F1=0.5710\n",
      "  Combo 50/216: Gap=0.3439, Val F1=0.6186\n",
      "  Combo 60/216: Gap=0.3500, Val F1=0.5845\n",
      "  Combo 70/216: Gap=0.2871, Val F1=0.6589\n",
      "  Combo 80/216: Gap=0.3986, Val F1=0.5734\n",
      "  Combo 90/216: Gap=0.3109, Val F1=0.6681\n",
      "  Combo 100/216: Gap=0.3670, Val F1=0.6028\n",
      "  Combo 110/216: Gap=0.2665, Val F1=0.6804\n",
      "  Combo 120/216: Gap=0.3610, Val F1=0.5821\n",
      "  Combo 130/216: Gap=0.2551, Val F1=0.6713\n",
      "  Combo 140/216: Gap=0.3625, Val F1=0.5744\n",
      "  Combo 150/216: Gap=0.3012, Val F1=0.6713\n",
      "  Combo 160/216: Gap=0.3632, Val F1=0.6180\n",
      "  Combo 170/216: Gap=0.2980, Val F1=0.6667\n",
      "  Combo 180/216: Gap=0.3945, Val F1=0.5740\n",
      "  Combo 190/216: Gap=0.2991, Val F1=0.6693\n",
      "  Combo 200/216: Gap=0.4116, Val F1=0.5757\n",
      "  Combo 210/216: Gap=0.2927, Val F1=0.6785\n",
      "Selected params: {'n_estimators': 50, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 15, 'max_features': 'sqrt', 'max_samples': 0.8}\n",
      "Selected Random Forest model val F1: 0.6669, train-val acc gap: 0.2318\n",
      "\n",
      "Tuning and selecting SVM (RBF) with strict overfit prevention...\n",
      "Testing 9 parameter combinations...\n",
      "  Combo 1/9: Gap=0.0819, Val F1=0.3104\n",
      "Selected params: {'C': 0.01, 'gamma': 'scale'}\n",
      "Selected SVM (RBF) model val F1: 0.3104, train-val acc gap: 0.0819\n",
      "\n",
      "Tuning and selecting Gradient Boosting with strict overfit prevention...\n",
      "Testing 216 parameter combinations...\n",
      "  Combo 1/216: Gap=0.1225, Val F1=0.6515\n",
      "  Combo 10/216: Gap=0.0941, Val F1=0.6441\n",
      "  Combo 20/216: Gap=0.1880, Val F1=0.6663\n",
      "  Combo 30/216: Gap=0.2201, Val F1=0.6721\n",
      "  Combo 40/216: Gap=0.2324, Val F1=0.6916\n",
      "  Combo 50/216: Gap=0.2768, Val F1=0.6983\n",
      "  Combo 60/216: Gap=0.2763, Val F1=0.6958\n",
      "  Combo 70/216: Gap=0.2820, Val F1=0.7095\n",
      "  Combo 80/216: Gap=0.2845, Val F1=0.7010\n",
      "  Combo 90/216: Gap=0.2980, Val F1=0.6981\n",
      "  Combo 100/216: Gap=0.2892, Val F1=0.7076\n",
      "  Combo 110/216: Gap=0.1705, Val F1=0.6634\n",
      "  Combo 120/216: Gap=0.1312, Val F1=0.6644\n",
      "  Combo 130/216: Gap=0.2526, Val F1=0.6635\n",
      "  Combo 140/216: Gap=0.2805, Val F1=0.6815\n",
      "  Combo 150/216: Gap=0.2742, Val F1=0.7075\n",
      "  Combo 160/216: Gap=0.2809, Val F1=0.7141\n",
      "  Combo 170/216: Gap=0.2623, Val F1=0.7324\n",
      "  Combo 180/216: Gap=0.2747, Val F1=0.7208\n",
      "  Combo 190/216: Gap=0.2830, Val F1=0.7168\n",
      "  Combo 200/216: Gap=0.2582, Val F1=0.7389\n",
      "  Combo 210/216: Gap=0.2769, Val F1=0.7204\n",
      "Selected params: {'n_estimators': 50, 'learning_rate': 0.01, 'max_depth': 2, 'subsample': 0.8, 'min_samples_leaf': 5, 'min_samples_split': 10}\n",
      "Selected Gradient Boosting model val F1: 0.6445, train-val acc gap: 0.0904\n",
      "\n",
      "Training stacked learner...\n",
      "PCA reduced features from 802816 to 344\n",
      "Training base learner: Logistic Regression\n",
      "Training base learner: Random Forest\n",
      "Training base learner: SVM (RBF)\n",
      "Training base learner: Gradient Boosting\n",
      "\n",
      "Stacked Learner Results for middle features:\n",
      "Train Accuracy: 1.0000\n",
      "Test Accuracy: 0.8361\n",
      "Test F1 Macro: 0.8324\n",
      "Overfitting Gap: 0.1639\n",
      "Base learners used: Logistic Regression, Random Forest, SVM (RBF), Gradient Boosting\n",
      "\n",
      "==================================================\n",
      "PHASE 2 - STACKED LEARNER FOR HIGH FEATURES\n",
      "==================================================\n",
      "PCA reduced features from 100352 to 321\n",
      "\n",
      "Tuning and selecting Logistic Regression with strict overfit prevention...\n",
      "Testing 8 parameter combinations...\n",
      "  Combo 1/8: Gap=0.0165, Val F1=0.9835\n",
      "Selected params: {'C': 0.001, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Selected Logistic Regression model val F1: 0.9877, train-val acc gap: 0.0124\n",
      "\n",
      "Tuning and selecting Random Forest with strict overfit prevention...\n",
      "Testing 216 parameter combinations...\n",
      "  Combo 1/216: Gap=0.1053, Val F1=0.8776\n",
      "  Combo 10/216: Gap=0.0723, Val F1=0.8958\n",
      "  Combo 20/216: Gap=0.1750, Val F1=0.7861\n",
      "  Combo 30/216: Gap=0.0795, Val F1=0.8941\n",
      "  Combo 40/216: Gap=0.1750, Val F1=0.8166\n",
      "  Combo 50/216: Gap=0.0939, Val F1=0.8967\n",
      "  Combo 60/216: Gap=0.1750, Val F1=0.8012\n",
      "  Combo 70/216: Gap=0.0790, Val F1=0.8984\n",
      "  Combo 80/216: Gap=0.1746, Val F1=0.8190\n",
      "  Combo 90/216: Gap=0.1110, Val F1=0.8805\n",
      "  Combo 100/216: Gap=0.1734, Val F1=0.8101\n",
      "  Combo 110/216: Gap=0.0749, Val F1=0.9132\n",
      "  Combo 120/216: Gap=0.1415, Val F1=0.8193\n",
      "  Combo 130/216: Gap=0.0811, Val F1=0.9006\n",
      "  Combo 140/216: Gap=0.1673, Val F1=0.8016\n",
      "  Combo 150/216: Gap=0.0852, Val F1=0.9115\n",
      "  Combo 160/216: Gap=0.1890, Val F1=0.8066\n",
      "  Combo 170/216: Gap=0.0769, Val F1=0.9169\n",
      "  Combo 180/216: Gap=0.1559, Val F1=0.8284\n",
      "  Combo 190/216: Gap=0.0909, Val F1=0.9021\n",
      "  Combo 200/216: Gap=0.1596, Val F1=0.8369\n",
      "  Combo 210/216: Gap=0.0821, Val F1=0.9090\n",
      "Selected params: {'n_estimators': 100, 'max_depth': 8, 'min_samples_split': 20, 'min_samples_leaf': 5, 'max_features': 'sqrt', 'max_samples': 0.8}\n",
      "Selected Random Forest model val F1: 0.9404, train-val acc gap: 0.0579\n",
      "\n",
      "Tuning and selecting SVM (RBF) with strict overfit prevention...\n",
      "Testing 9 parameter combinations...\n",
      "  Combo 1/9: Gap=0.0447, Val F1=0.3124\n",
      "Selected params: {'C': 1, 'gamma': 'scale'}\n",
      "Selected SVM (RBF) model val F1: 0.9551, train-val acc gap: 0.0454\n",
      "\n",
      "Tuning and selecting Gradient Boosting with strict overfit prevention...\n",
      "Testing 216 parameter combinations...\n",
      "  Combo 1/216: Gap=0.0573, Val F1=0.9111\n",
      "  Combo 10/216: Gap=0.0557, Val F1=0.9110\n",
      "  Combo 20/216: Gap=0.0625, Val F1=0.9238\n",
      "  Combo 30/216: Gap=0.0475, Val F1=0.9213\n",
      "  Combo 40/216: Gap=0.0588, Val F1=0.9317\n",
      "  Combo 50/216: Gap=0.0583, Val F1=0.9401\n",
      "  Combo 60/216: Gap=0.0588, Val F1=0.9400\n",
      "  Combo 70/216: Gap=0.0475, Val F1=0.9523\n",
      "  Combo 80/216: Gap=0.0557, Val F1=0.9443\n",
      "  Combo 90/216: Gap=0.0557, Val F1=0.9444\n",
      "  Combo 100/216: Gap=0.0599, Val F1=0.9403\n",
      "  Combo 110/216: Gap=0.0676, Val F1=0.9068\n",
      "  Combo 120/216: Gap=0.0506, Val F1=0.9152\n",
      "  Combo 130/216: Gap=0.0547, Val F1=0.9382\n",
      "  Combo 140/216: Gap=0.0609, Val F1=0.9361\n",
      "  Combo 150/216: Gap=0.0516, Val F1=0.9487\n",
      "  Combo 160/216: Gap=0.0557, Val F1=0.9445\n",
      "  Combo 170/216: Gap=0.0619, Val F1=0.9381\n",
      "  Combo 180/216: Gap=0.0475, Val F1=0.9526\n",
      "  Combo 190/216: Gap=0.0537, Val F1=0.9466\n",
      "  Combo 200/216: Gap=0.0516, Val F1=0.9485\n",
      "  Combo 210/216: Gap=0.0599, Val F1=0.9400\n",
      "Selected params: {'n_estimators': 50, 'learning_rate': 0.1, 'max_depth': 3, 'subsample': 0.8, 'min_samples_leaf': 10, 'min_samples_split': 10}\n",
      "Selected Gradient Boosting model val F1: 0.9567, train-val acc gap: 0.0433\n",
      "\n",
      "Training stacked learner...\n",
      "PCA reduced features from 100352 to 321\n",
      "Training base learner: Logistic Regression\n",
      "Training base learner: Random Forest\n",
      "Training base learner: SVM (RBF)\n",
      "Training base learner: Gradient Boosting\n",
      "\n",
      "Stacked Learner Results for high features:\n",
      "Train Accuracy: 1.0000\n",
      "Test Accuracy: 0.9836\n",
      "Test F1 Macro: 0.9835\n",
      "Overfitting Gap: 0.0164\n",
      "Base learners used: Logistic Regression, Random Forest, SVM (RBF), Gradient Boosting\n",
      "\n",
      "============================================================\n",
      "PHASE 2 RESULTS SUMMARY\n",
      "============================================================\n",
      "\n",
      "Stacked Learner Performance:\n",
      "       feature_type test_accuracy test_f1_macro overfitting_gap\n",
      "early         early      0.614754      0.556432          0.3088\n",
      "middle       middle      0.836066      0.832362        0.163934\n",
      "high           high      0.983607      0.983537        0.016393\n",
      "\n",
      "Overfitting Analysis:\n",
      "early: Gap = 0.3088 - ✗ Overfitting\n",
      "middle: Gap = 0.1639 - ✗ Overfitting\n",
      "high: Gap = 0.0164 - ✓ Good\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.base import clone\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def load_dataset_labels(dataset_path):\n",
    "    class_names = sorted(os.listdir(dataset_path))\n",
    "    class_to_idx = {cls: idx for idx, cls in enumerate(class_names)}\n",
    "    labels = []\n",
    "    for cls in class_names:\n",
    "        cls_folder = os.path.join(dataset_path, cls)\n",
    "        if not os.path.isdir(cls_folder): continue\n",
    "        for img_file in os.listdir(cls_folder):\n",
    "            if img_file.lower().endswith(('jpg', 'jpeg', 'png')):\n",
    "                labels.append(class_to_idx[cls])\n",
    "    return np.array(labels), class_names\n",
    "\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    return {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'f1_macro': f1_score(y_true, y_pred, average='macro')\n",
    "    }\n",
    "\n",
    "def tune_with_overfit_check(classifier, param_grid, X_train, y_train, overfit_thresh=0.8, cv=5):\n",
    "    \"\"\"\n",
    "    Stricter overfitting prevention with lower threshold and more CV folds.\n",
    "    \"\"\"\n",
    "    best_model = None\n",
    "    best_val_f1 = -np.inf\n",
    "    best_gap = np.inf\n",
    "    best_params = None\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "\n",
    "    # Iterate all param combos manually\n",
    "    import itertools\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    param_combos = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "    \n",
    "    print(f\"Testing {len(param_combos)} parameter combinations...\")\n",
    "\n",
    "    for i, params in enumerate(param_combos):\n",
    "        val_f1_scores = []\n",
    "        val_acc_scores = []\n",
    "        train_acc_scores = []\n",
    "\n",
    "        for train_idx, val_idx in skf.split(X_train, y_train):\n",
    "            X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "            y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "            model = clone(classifier).set_params(**params)\n",
    "            model.fit(X_tr, y_tr)\n",
    "\n",
    "            y_tr_pred = model.predict(X_tr)\n",
    "            y_val_pred = model.predict(X_val)\n",
    "\n",
    "            train_acc = accuracy_score(y_tr, y_tr_pred)\n",
    "            val_acc = accuracy_score(y_val, y_val_pred)\n",
    "            val_f1 = f1_score(y_val, y_val_pred, average='macro')\n",
    "\n",
    "            train_acc_scores.append(train_acc)\n",
    "            val_acc_scores.append(val_acc)\n",
    "            val_f1_scores.append(val_f1)\n",
    "\n",
    "        mean_train_acc = np.mean(train_acc_scores)\n",
    "        mean_val_acc = np.mean(val_acc_scores)\n",
    "        mean_val_f1 = np.mean(val_f1_scores)\n",
    "        acc_gap = mean_train_acc - mean_val_acc\n",
    "\n",
    "        # Print progress for monitoring\n",
    "        if (i + 1) % 10 == 0 or i == 0:\n",
    "            print(f\"  Combo {i+1}/{len(param_combos)}: Gap={acc_gap:.4f}, Val F1={mean_val_f1:.4f}\")\n",
    "\n",
    "        # Strict selection: only accept models with low overfitting\n",
    "        if acc_gap <= overfit_thresh:\n",
    "            if mean_val_f1 > best_val_f1:\n",
    "                best_model = clone(classifier).set_params(**params)\n",
    "                best_val_f1 = mean_val_f1\n",
    "                best_gap = acc_gap\n",
    "                best_params = params\n",
    "        if best_model is None or (acc_gap< best_gap and best_gap> overfit_thresh):\n",
    "            best_model = clone(classifier).set_params(**params)\n",
    "            best_val_f1 = mean_val_f1\n",
    "            best_gap = acc_gap\n",
    "            best_params = params\n",
    "\n",
    "\n",
    "    print(f\"Selected params: {best_params}\")\n",
    "    best_model.fit(X_train, y_train)  # retrain on full training set\n",
    "    return best_model, best_val_f1, best_gap\n",
    "\n",
    "class StackedLearner:\n",
    "    def __init__(self, base_learners, meta_learner, use_original_features=False):\n",
    "        self.base_learners = base_learners\n",
    "        self.meta_learner = meta_learner\n",
    "        self.use_original_features = use_original_features  # Changed default to False\n",
    "        self.scaler = StandardScaler()\n",
    "        self.pca = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        \n",
    "        # More aggressive PCA for overfitting prevention\n",
    "        if X_scaled.shape[1] > 500:  # Lower threshold\n",
    "            self.pca = PCA(n_components=0.85, random_state=42)  # Keep less variance\n",
    "            X_processed = self.pca.fit_transform(X_scaled)\n",
    "            print(f\"PCA reduced features from {X_scaled.shape[1]} to {X_processed.shape[1]}\")\n",
    "        else:\n",
    "            X_processed = X_scaled\n",
    "        \n",
    "        base_predictions = []\n",
    "        for name, learner in self.base_learners.items():\n",
    "            print(f\"Training base learner: {name}\")\n",
    "            learner.fit(X_processed, y)\n",
    "            # Use cross-val predictions for stacking\n",
    "            from sklearn.model_selection import cross_val_predict\n",
    "            cv_preds = cross_val_predict(learner, X_processed, y, cv=5, method='predict_proba')\n",
    "            base_predictions.append(cv_preds)\n",
    "        \n",
    "        stacked_features = np.column_stack(base_predictions)\n",
    "        \n",
    "        # Reduced feature inclusion to prevent overfitting\n",
    "        if self.use_original_features and X_processed.shape[1] < 100:\n",
    "            stacked_features = np.column_stack([stacked_features, X_processed])\n",
    "        \n",
    "        self.meta_learner.fit(stacked_features, y)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        if self.pca:\n",
    "            X_processed = self.pca.transform(X_scaled)\n",
    "        else:\n",
    "            X_processed = X_scaled\n",
    "            \n",
    "        base_predictions = [learner.predict_proba(X_processed) for learner in self.base_learners.values()]\n",
    "        stacked_features = np.column_stack(base_predictions)\n",
    "        \n",
    "        if self.use_original_features and X_processed.shape[1] < 100:\n",
    "            stacked_features = np.column_stack([stacked_features, X_processed])\n",
    "            \n",
    "        return self.meta_learner.predict(stacked_features)\n",
    "\n",
    "# ---- Main phase2 training & tuning loop ----\n",
    "\n",
    "# Load labels and features (adjust paths accordingly)\n",
    "labels, class_names = load_dataset_labels('dataset')\n",
    "print(f\"Loaded {len(labels)} labels for {len(class_names)} classes: {class_names}\")\n",
    "\n",
    "feature_types = ['early', 'middle', 'high']\n",
    "extracted_features = {}\n",
    "for ftype in feature_types:\n",
    "    feat_path = f\"saved_features/{ftype}_features.npy\"\n",
    "    if os.path.exists(feat_path):\n",
    "        extracted_features[ftype] = np.load(feat_path)\n",
    "        print(f\"Loaded {ftype} features: {extracted_features[ftype].shape}\")\n",
    "    else:\n",
    "        print(f\"Warning: {feat_path} not found!\")\n",
    "\n",
    "results_phase2 = {}\n",
    "\n",
    "# More regularized parameter grids to prevent overfitting\n",
    "param_grids = {\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [50, 100],\n",
    "        'max_depth': [3, 5, 8],  # Reduced max depth\n",
    "        'min_samples_split': [10, 20, 30],  # Increased min samples\n",
    "        'min_samples_leaf': [5, 10, 15],  # Increased min samples\n",
    "        'max_features': ['sqrt', 'log2'],  # Removed 'auto' which can cause overfitting\n",
    "        'max_samples': [0.6, 0.8]  # Added bootstrap sampling limit\n",
    "    },\n",
    "    'SVM (RBF)': {\n",
    "        'C': [0.01, 0.1, 1],  # Reduced C values for more regularization\n",
    "        'gamma': ['scale', 0.001, 0.01],  # Added smaller gamma values\n",
    "    },\n",
    "    'Logistic Regression': {\n",
    "        'C': [0.001, 0.01, 0.1, 1],  # More regularization\n",
    "        'penalty': ['l2'],\n",
    "        'solver': ['liblinear', 'lbfgs']\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'n_estimators': [50, 100],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],  # Added lower learning rate\n",
    "        'max_depth': [2, 3, 4],  # Reduced max depth\n",
    "        'subsample': [0.6, 0.8],  # Reduced subsample ratios\n",
    "        'min_samples_leaf': [5, 10, 15],  # Increased min samples\n",
    "        'min_samples_split': [10, 20]  # Added min samples split\n",
    "    }\n",
    "}\n",
    "\n",
    "for ftype in feature_types:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"PHASE 2 - STACKED LEARNER FOR {ftype.upper()} FEATURES\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "    X = extracted_features[ftype]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # More aggressive dimensionality reduction\n",
    "    if X_train_scaled.shape[1] > 500:\n",
    "        pca = PCA(n_components=0.85, random_state=42)  # Keep less variance\n",
    "        X_train_reduced = pca.fit_transform(X_train_scaled)\n",
    "        X_test_reduced = pca.transform(X_test_scaled)\n",
    "        print(f\"PCA reduced features from {X_train_scaled.shape[1]} to {X_train_reduced.shape[1]}\")\n",
    "    else:\n",
    "        X_train_reduced = X_train_scaled\n",
    "        X_test_reduced = X_test_scaled\n",
    "\n",
    "    base_learners = {\n",
    "        'Logistic Regression': LogisticRegression(random_state=42, max_iter=200),\n",
    "        'Random Forest': RandomForestClassifier(random_state=42),\n",
    "        'SVM (RBF)': SVC(kernel='rbf', probability=True, random_state=42),\n",
    "        'Gradient Boosting': GradientBoostingClassifier(random_state=42)\n",
    "    }\n",
    "\n",
    "    tuned_learners = {}\n",
    "\n",
    "    for name, learner in base_learners.items():\n",
    "        print(f\"\\nTuning and selecting {name} with strict overfit prevention...\")\n",
    "        if name in param_grids:\n",
    "            tuned_model, val_f1, acc_gap = tune_with_overfit_check(\n",
    "                learner, param_grids[name], X_train_reduced, y_train, \n",
    "                overfit_thresh=0.05, cv=5)  # Stricter threshold and more CV folds\n",
    "            print(f\"Selected {name} model val F1: {val_f1:.4f}, train-val acc gap: {acc_gap:.4f}\")\n",
    "            tuned_learners[name] = tuned_model\n",
    "        else:\n",
    "            learner.fit(X_train_reduced, y_train)\n",
    "            tuned_learners[name] = learner\n",
    "\n",
    "    # Use more regularized meta-learner\n",
    "    meta_learner = LogisticRegression(random_state=42, max_iter=200, C=0.1)\n",
    "    stacked_model = StackedLearner(tuned_learners, meta_learner, use_original_features=False)\n",
    "\n",
    "    print(\"\\nTraining stacked learner...\")\n",
    "    stacked_model.fit(X_train, y_train)\n",
    "\n",
    "    y_train_pred = stacked_model.predict(X_train)\n",
    "    y_test_pred = stacked_model.predict(X_test)\n",
    "\n",
    "    train_metrics = evaluate_model(y_train, y_train_pred)\n",
    "    test_metrics = evaluate_model(y_test, y_test_pred)\n",
    "\n",
    "    # Calculate final overfitting gap\n",
    "    overfitting_gap = train_metrics['accuracy'] - test_metrics['accuracy']\n",
    "\n",
    "    results_phase2[ftype] = {\n",
    "        'feature_type': ftype,\n",
    "        'train_accuracy': train_metrics['accuracy'],\n",
    "        'train_f1_macro': train_metrics['f1_macro'],\n",
    "        'test_accuracy': test_metrics['accuracy'],\n",
    "        'test_f1_macro': test_metrics['f1_macro'],\n",
    "        'overfitting_gap': overfitting_gap,\n",
    "        'base_learners': list(tuned_learners.keys())\n",
    "    }\n",
    "\n",
    "    print(f\"\\nStacked Learner Results for {ftype} features:\")\n",
    "    print(f\"Train Accuracy: {train_metrics['accuracy']:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "    print(f\"Test F1 Macro: {test_metrics['f1_macro']:.4f}\")\n",
    "    print(f\"Overfitting Gap: {overfitting_gap:.4f}\")\n",
    "    print(f\"Base learners used: {', '.join(tuned_learners.keys())}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"PHASE 2 RESULTS SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "phase2_df = pd.DataFrame(results_phase2).T\n",
    "print(\"\\nStacked Learner Performance:\")\n",
    "print(phase2_df[['feature_type', 'test_accuracy', 'test_f1_macro', 'overfitting_gap']])\n",
    "\n",
    "# Check if overfitting is resolved\n",
    "print(f\"\\nOverfitting Analysis:\")\n",
    "for ftype in feature_types:\n",
    "    gap = results_phase2[ftype]['overfitting_gap']\n",
    "    status = \"✓ Good\" if gap <= 0.05 else \"⚠ Needs attention\" if gap <= 0.10 else \"✗ Overfitting\"\n",
    "    print(f\"{ftype}: Gap = {gap:.4f} - {status}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
